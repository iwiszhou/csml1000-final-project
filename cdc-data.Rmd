---
title: Tackling Obesity — Prioritizing the Risk Factors by Demographic
output:
  html_document: default
  # pdf_document: default - coloured graphs take way too long to render in PDF form.
---

# CSML1000 Final Project, by Group 8
Tamer Hanna <tamerh@my.yorku.ca>
Pete Gray <ptgray@my.yorku.ca>
Xiaohai Lu <yu271637@my.yorku.ca>
Haofeng Zhou <zhf85@my.yorku.ca>



# OVERVIEW

Obesity is a big problem in North American society, and contributes to increased rates of diabetes, heart disease, stroke, depression, suicide, disability, and death. All of these complications reduce quality of life and cause additional burden on the health care system.

Using this dataset, we can identify groups at highest risk based on age, gender, race, education, income, etc. Using unsupervised learning, we can look for unexpected combinations of features that lead to an increased rate of obesity. Using decision trees, we can divine the most critical factors correlated with high rates of obesity. We can do this in general, or within those clusters. Using predictive modeling, we can predict obesity rates within demographics using real, or hypothetically modified values to represent predicted improvements in obesity rates in response to hypothetical lifestyle improvements within demographics. 

Within the data are a wide array of factors known to be associated with increased incidence of obesity. Some of these factors, such as ethnic background, do have predictive value with regard to obesity rates. They are, however, not factors that can be addressed. There are also factors such as eating and exercise, which are very strongly correlated with obesity and can to some small degree be addressed though public awareness campaigns and other marketing-like activities. And there are other factors, such as education and income, which are both correlated and can be addressed, but only in the very long term.

We wish, then, to explore which addressable factors are most significant, for any demographic defined by factors that are more inherent.



# Business Objectives

For any given demographic, we would like to be able to predict the impact of changing different factors on that segment's obesity rate.

This knowledge would be of value to any organizations with marketing channels into specific demographic groups, who wish to promote the kind lifestyle modifications most likely to reduce obesity rates for that group. Does focusing on eating or exercise improve obesity rates more in poorly educated young men? In well-educated middle-aged women? In senior citizens? How would getting a better education in the first place compare to difficult lifestyle modifications now?

As we explore the data, and look for patterns, and learn how to make predictions based on modifying individual features within any given group, we can find answers to these questions and more. We would then work to communicate those findings to people outside the field of Data Science, who could then harness these discoveries into the optimization of a wide variety of targeted public health initiatives.


#### Load Libraries

```{r message=FALSE, warning=FALSE}
library(dplyr);
library(ggplot2);
library(knitr);
library(validate);
library(tidyverse);  # data manipulation
library(cluster);    # clustering algorithms
library(clusterSim);
library(factoextra);
library(fpc);
library(flexclust);
```

## Collect Initial Data

Load the data from the local filesystem:

```{r message=FALSE, warning=FALSE}
#load data
df <- read_csv("dataset.csv");
df_origin <- df
```

Output a quick and dirty summary of the dataset, to ensure that we haven't loaded something scrambled, or the wrong thing:

```{r message=FALSE, warning=FALSE}
str(df);
head(df);
```

## Describe Data

#### The data describe 9 different questions and each one has a corresponding precentage which is Data_Value column

1. Percent of adults aged 18 years and older who have an overweight classification                                             
2. Percent of adults aged 18 years and older who have obesity                                                                 
3. Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic activity (or a…
4. Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic physical activ…
5. Percent of adults who achieve at least 300 minutes a week of moderate-intensity aerobic physical activity or 150 minutes a week of vigorous-intensity aerobic activity (or …
6. Percent of adults who engage in muscle-strengthening activities on 2 or more days a week                                   7. Percent of adults who engage in no leisure-time physical activity                                                          8. Percent of adults who report consuming fruit less than one time daily                                                      9. Percent of adults who report consuming vegetables less than one time daily

#### For each of there questions, there are 6 different categories

1. Age (years)            
2. Education              
3. Gender                 
4. Income                 
5. Race/Ethnicity         
6. Total

#### For each of there categories, which has different values

1. Age (years)	(6 levels)
  - 18 - 24
  - 25 - 34
  - 35 - 44
  - 45 - 54
  - 55 - 64
  - 65 or older

2. Education	(4 levels)
  - Less than high school
  - High school graduate
  - Some college or technical school
  - College graduate

3. Gender (2 levels)
  - Male
  - Female

4. Income (7 levels)
  - Less than $15,000
  - $15,000 - $24,999
  - $25,000 - $34,999
  - $35,000 - $49,999
  - $50,000 - $74,999
  - $75,000 or greater
  - Data not reported

5. Race/Ethnicity	(8 levels)
  - 2 or more races
  - American Indian/Alaska Native			
  - Asian			
  - Hawaiian/Pacific Islander			
  - Hispanic			
  - Non-Hispanic Black			
  - Non-Hispanic White			
  - Other

6. Total (1 levels)
  - Total

## Data cleaning

At first glance, some columns are just described another. Let's look at one by one from left to right. And select the necessary columns.
Firs of all, converting all character to factor

```{r message=FALSE, warning=FALSE}
df$LocationAbbr <- as.factor(df$LocationAbbr)
df$LocationDesc <- as.factor(df$LocationDesc)
df$Datasource <- as.factor(df$Datasource)
df$Class <- as.factor(df$Class)
df$Topic <- as.factor(df$Topic)
df$Data_Value_Unit <- as.factor(df$Data_Value_Unit)
df$Data_Value_Type <- as.factor(df$Data_Value_Type)
df$Data_Value_Footnote_Symbol <- as.factor(df$Data_Value_Footnote_Symbol)
df$Data_Value_Footnote <- as.factor(df$Data_Value_Footnote)
df$Total <- as.factor(df$Total)
df$Education <- as.factor(df$Education)
df$Gender <- as.factor(df$Gender)
df$Income <- as.factor(df$Income)
df$ClassID <- as.factor(df$ClassID)
df$TopicID <- as.factor(df$TopicID)
df$QuestionID <- as.factor(df$QuestionID)
df$DataValueTypeID <- as.factor(df$DataValueTypeID)
df$LocationID <- as.factor(df$LocationID)
df$DataValueTypeID <- as.factor(df$DataValueTypeID)
df$StratificationCategory1 <- as.factor(df$StratificationCategory1)
df$Stratification1 <- as.factor(df$Stratification1)
df$StratificationCategoryId1 <- as.factor(df$StratificationCategoryId1)
df$StratificationID1 <- as.factor(df$StratificationID1)
```

Next, walk through each column.

#### YearStart vs YearEnd

Group by YearStart and YearEnd, if they are same, we can keep one and remove the other.

```{r message=FALSE, warning=FALSE}
df %>% group_by(YearStart,YearEnd) %>% summarise(count=n())
```

We can see YearStart and YearEnd are always the same. let's remove YearEnd.
And we also found each year contains different size of data. 2015 > 2013 > 2011 > 2016 > 2014 > 2012

```{r message=FALSE, warning=FALSE}
df$YearEnd <- NULL
```

#### Datasource

Datasource has only one level. We can remove it as well.

```{r message=FALSE, warning=FALSE}
df$Datasource <- NULL
```

#### Class vs Topic, ClassID vs TopicID

Class and Topic have 3 levels. Let's take a look whether those 3 level are same or not.

```{r message=FALSE, warning=FALSE}
levels(df$Class)
levels(df$Topic)
```

Class levels : Fruits and Vegetables, Obesity / Weight Status, Physical Activity
Topic levels: Fruits and Vegetables - Behavior,  Obesity / Weight Status,  Physical Activity - Behavior
They are identical, so that we can remove one, which is Topic

```{r message=FALSE, warning=FALSE}
df$Topic <- NULL
```

ClassID and TopicID columms are just abbr/transform value of Class and Topic colums. We can continous Class column and ignore there two.

```{r message=FALSE, warning=FALSE}
df$ClassID <- NULL
df$TopicID <- NULL
```

#### Question vs QuestionID

Again, QuestionID is a value tranfromation of Question column. We keep Question and remove Question.

```{r message=FALSE, warning=FALSE}
df %>% group_by(QuestionID, Question) %>% summarise()
df$Question <- NULL
```

There are total 9 levels, which means this dataset might be able to split by 9 different group. Below is the mapping:
Q018 - Percent of adults who report consuming fruit less than one time daily
Q019 - Percent of adults who report consuming vegetables less than one time daily
Q036 - Percent of adults aged 18 years and older who have obesity
Q037 - Percent of adults aged 18 years and older who have an overweight classification
Q043 - Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic activity (or an equivalent combination)
Q044 - Percent of adults who achieve at least 150 minutes a week of moderate-intensity aerobic physical activity or 75 minutes a week of vigorous-intensity aerobic physical activity and engage in muscle-strengthening activities on 2 or more days a week
Q045 - Percent of adults who achieve at least 300 minutes a week of moderate-intensity aerobic physical activity or 150 minutes a week of vigorous-intensity aerobic activity (or an equivalent combination)
Q046 - Percent of adults who engage in muscle-strengthening activities on 2 or more days a week
Q047 - Percent of adults who engage in no leisure-time physical activity

According to each question, it is obviously Q036 and Q037 are relative to weight status/obesity problem.
Q018 and Q019 are relative to healthy diet, fruit and vegetable consuming.
Q043, Q044, Q045, Q046 and Q047 are relative to physical activity.

For this final project, we will foucus on obesity rate, which is relative to questions Q036 and Q037.

#### Data_Value_Unit vs Data_Value_Type vs DataValueTypeID

Since Data_Value_Unit is all NA. And Data_Value_Type and DataValueTypeID are all "VALUE". We can remove them.

```{r message=FALSE, warning=FALSE}
df$Data_Value_Unit <- NULL
df$Data_Value_Type <- NULL
df$DataValueTypeID <- NULL
```

#### Data_Value vs Data_Value_Alt

Based on first glance, there 2 columns should be identical. Let's use group by to confirm.

```{r message=FALSE, warning=FALSE}
df %>% group_by(Data_Value,Data_Value_Alt)
```

After group by, total rows is 53,392, which is same as original dataset. So that, they are the same. Let's remove Data_Value_Alt.

```{r message=FALSE, warning=FALSE}
df$Data_Value_Alt <- NULL
```

#### Data_Value vs Low_Confidence_Limit vs High_Confidence_Limit

High_Confidence_Limit and Low_Confidence_Limit. These two columns are not very clear what is actually representing. 
Based the on column name, it looks like the min and max value of the Data_Value in each group. 
So that, we can remove two columns for now. And we would reley on Data_Value more than those High_Confidence_Limit and Low_Confidence_Limit

```{r message=FALSE, warning=FALSE}
df$Low_Confidence_Limit <- NULL;
df$High_Confidence_Limit <- NULL;
```

#### Data_Value_Footnote_Symbol vs Data_Value_Footnote

According to the name, these 2 columns are the same. when "Data_Value_Footnote_Symbol = ~" is equal "Data_Value_Footnote_Symbol = Data not available because sample size is insufficient."
Let's filter out when Data_Value_Footnote_Symbol is ~.

```{r message=FALSE, warning=FALSE}
head(df %>% filter(Data_Value_Footnote_Symbol == "~"))
```

When Data_Value_Footnote_Symbol is ~, Data_Value value will be NA. We need to clean data. So that, we need to remove those records.

```{r message=FALSE, warning=FALSE}
df <- df %>% filter(is.na(Data_Value_Footnote_Symbol))
```

After that, we can drop Data_Value_Footnote_Symbol and Data_Value_Footnote.

```{r message=FALSE, warning=FALSE}
df$Data_Value_Footnote_Symbol <- NULL
df$Data_Value_Footnote <- NULL
```

#### Total, Age(years), Education, Gender, Income and Race/Ethnicity vs StratificationCategory1 and Stratification1

Let's take a look the relationship between these columns.

```{r message=FALSE, warning=FALSE}
head(df,5);
```

According to the data, it is easy to find out columns "Total, Age(years), Education, Gender, Income and Race/Ethnicity", only one column has value. 
For example, if column "Total" has value, columns "Education, Gender, Income and Race/Ethnicity"'s value are NA. And StratificationCategory1's value is "Total".
If column "Gender" has value, columns "Education, Total, Income and Race/Ethnicity"'s value are NA. And StratificationCategory1's value is "Gender". Column Stratification1's value is "MALE" or "FEMALE".
Thereforce, StratificationCategory1 is summary of columns "Total, Age(years), Education, Gender, Income and Race/Ethnicity". And Stratification1 colmn is the actual value.
In this case, we can reply on "StratificationCategory1 and Stratification1". And remove "Total, Age(years), Education, Gender, Income and Race/Ethnicity"

```{r message=FALSE, warning=FALSE}
df$Total <- NULL
df$`Age(years)` <- NULL
df$Education <- NULL
df$Gender <- NULL
df$Income <- NULL
df$`Race/Ethnicity` <- NULL
```

Let's group by StratificationCategory1 and Stratification1 and see how many different combinations in the dataset.
```{r message=FALSE, warning=FALSE}
df %>% group_by(StratificationCategory1) %>% summarise()
df %>% group_by(StratificationCategory1) %>% summarise(CategoryDistinctValue = n_distinct(Stratification1))
```

We can see there are 6 categories. And each category might contain more than value.
For example, in "Age(years)" category, it has 6 different values.

#### StratificationCategoryId1 vs StratificationCategoryId1

StratificationCategoryId1 vs StratificationCategoryId1 are the transform value for StratificationCategory1 vs StratificationCategory1. 
So that, we can remove them.

```{r message=FALSE, warning=FALSE}
df$StratificationCategoryId1 <- NULL
df$StratificationID1 <- NULL
```

#### LocationID vs LocationDesc vs LocationAbbr vs GeoLocation 

Based on columns' name, they should represent the same thing. Let's group by them and see whether we can remove the redundant columns or not.

```{r message=FALSE, warning=FALSE}
df %>% group_by(LocationID, LocationAbbr, GeoLocation, LocationDesc) %>% summarise()
```

After group by, we can clearly to see there are identical and repreesnt in different way. We can keep LocationAbbr and remove others.

```{r message=FALSE, warning=FALSE}
df$LocationDesc <- NULL
df$GeoLocation <- NULL
df$LocationID <- NULL
```

Some of Location are not including in US's 50+ states or some data is crossing the whole US. We need to remove those as well.
```{r message=FALSE, warning=FALSE}
removeLocation <- c("US","GU","PR","VI");
df <- df %>% filter(!LocationAbbr %in% removeLocation);
```

#### Data Cleaning Round 1 - Summary

Let's check the dataset again and see what does the dataset looks like after first round cleaning

```{r message=FALSE, warning=FALSE}
str(df)
```

And let have a quick check whether any NA value in our dataset or not.

```{r message=FALSE, warning=FALSE}
apply(df, 2, function(x) any(is.na(x) | is.infinite(x)))
```

Great. There is no NA value found in our dataset. After dataset clean up, we have 10 columns and 48346 records(rows). Comparing to original dataset, we removed 23 columns and 5046 records(rows).

After the data clean up, let's explore data and try to find out some patterns.

## Explore Data

We'll plot some basic graphs, to ensure that the data conform to our limited domain understanding.

YearStart vs Data_Value - we expect there should a pattern between these 2 columns. Either increase by each year or decrease.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(x=YearStart, y=Data_Value, color=YearStart)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="YearStart vs Data_Value", x="YearStart", y="Data_Value") 
```

However, we can not find a linear relationship. How about if we consider Quesiton in this graph. It might show a clear pattern ?

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(x=YearStart, y=Data_Value, color=YearStart)) + 
  geom_point() + 
  facet_wrap(~QuestionID) + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="YearStart vs Data_Value", x="YearStart", y="Data_Value");
```

This graph tells us a very important information. Not all questions have a data from 2011-2016. Some of them are missing data in 2012, 2014 and 2016. 
Maybe we can select different questions such as Q036 and Q037 becuase they have full data from 2011-2016 and they are all about obesity problem.
We could alos choose a single year, such as 2015 all questions have full data in year 2015.(We could invest this one if we have more time.)
First of all, let's choose Q036 and Q037 and take a look the relationship for each categories.

#### Obesity Question Analysis 
####  - Q036 - Percent of adults aged 18 years and older who have obesity. 
####  - Q037 - Percent of adults aged 18 years and older who have an overweight classification

Let's create a new dataset and only contains data relative to Q036 and Q037.

```{r message=FALSE, warning=FALSE}
q1 <- df %>% filter(QuestionID %in% c("Q036","Q037") );
```

##### Year vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1 %>% ggplot(aes(x=YearStart, y=Data_Value, shape = StratificationCategory1)) + 
  geom_point(aes(colour = StratificationCategory1), size = 1) + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="YearStart vs Data_Value", x="YearStart", y="Data_Value");
```

We still can see a linear pattern between year and data value. It increases each year.

##### Categories vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1 %>% ggplot(aes(x=StratificationCategory1, y=Data_Value, color = Stratification1 ) ) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  theme(axis.text.x = element_text(angle=45)) +
  labs(title="All Category vs Data_Value", x="All Catetory", y="Obesity percentage")
```

Different category has different perceatage of obesity. Race/Ethnicity category has wider range than others. Age category has a high percetage of obesity, it is all green starting from 30 in y axis. Let's take look into each category.

##### Category - Age (years) vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1_age <- q1 %>% filter(StratificationCategory1 == "Age (years)");
q1_age %>% ggplot(aes(x=Stratification1, y=Data_Value, color=Stratification1) ) + 
  geom_boxplot() +
  facet_wrap(~ YearStart) +
  theme(axis.text.x = element_text(angle=45)) +
  stat_summary(fun.y=mean, geom="line", aes(group=1))  + 
  stat_summary(fun.y=mean, geom="point") +
  labs(title="Category Age vs Obesity", x="Age", y="Obesity percentage")
```

The graph is a little bit hard to find out which one has highest rate. Let's take a look the mean values.

```{r message=FALSE, warning=FALSE}
q1_age %>% group_by(Stratification1) %>% dplyr::summarize(Mean = mean(Data_Value, na.rm=TRUE))
```

It is clear obesity rate increases when age increase. Peek is around 55 - 64. And it drops after.

##### Category - Education vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1_education <- q1 %>% filter(StratificationCategory1 == "Education");
q1_education %>% ggplot(aes(x=Stratification1, y=Data_Value,color=Stratification1)) + 
  geom_boxplot(position = position_dodge2(preserve = "total")) +
  facet_wrap(~ YearStart, ncol=3) +
  theme(axis.text.x = element_text(angle=45)) +
  coord_cartesian(ylim=c(20,38)) + 
  stat_summary(fun.y=mean, geom="line", aes(group=1))  + 
  stat_summary(fun.y=mean, geom="point") +
  labs(title="Category Education vs Obesity percentage", x="Education", y="Obesity percentage")
```

The graph is a little bit hard to find out which one has highest rate. Let's take a look the mean values.

```{r message=FALSE, warning=FALSE}
q1_education %>% group_by(Stratification1) %>% dplyr::summarize(Mean = mean(Data_Value, na.rm=TRUE))
```

This graph tell us lower educaiton level(High school graduate & Less than high school) seems has higher obesity rate. According to the mean value, we can say peole has Less than high school education has highest obesity rate.

##### Category - Income vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1_income <- q1 %>% filter(StratificationCategory1 == "Income");
q1_income %>% ggplot(aes(x=Stratification1, y=Data_Value, color=Stratification1) ) + 
  geom_boxplot() +
  facet_wrap(~ YearStart) +
  theme(axis.text.x = element_text(angle=45)) +
  coord_cartesian(ylim=c(21,40)) + 
  stat_summary(fun.y=mean, geom="line", aes(group=1))  + 
  stat_summary(fun.y=mean, geom="point") +
  labs(title="Category Income vs Obesity percentage", x="Income", y="Obesity percentage")
```

The graph is a little bit hard to find out which one has highest rate. Let's take a look the mean values.

```{r message=FALSE, warning=FALSE}
q1_income %>% group_by(Stratification1) %>% dplyr::summarize(Mean = mean(Data_Value, na.rm=TRUE))
```

Mean value tells us two boundaries case ($75,000+ and $15,000-) has lowest mean obesity rate. 
However, people has income between $75,000 or greater, which has bigger range than others. And the mean value is increasing year by year.

##### Category - Race/Ethnicity vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1_race <- q1 %>% filter(StratificationCategory1 == "Race/Ethnicity");
q1_race %>% ggplot(aes(x=Stratification1, y=Data_Value, color=Stratification1) ) + 
  geom_boxplot() + 
  facet_wrap(~ YearStart) +
  coord_cartesian(ylim=c(0,50)) + 
  theme(axis.text.x = element_text(angle=45)) +
  stat_summary(fun.y=mean, geom="line", aes(group=1))  + 
  stat_summary(fun.y=mean, geom="point") +
  labs(title="Category Race/Ethnicity vs Obesity percentage", x="Race/Ethnicity", y="Obesity percentage")
```

It is very clear Asian people has lowest obesity rate.

##### Category - Gender vs Obesity Rate

```{r message=FALSE, warning=FALSE}
q1_gender <- q1 %>% filter(StratificationCategory1 == "Gender");
q1_gender %>% ggplot(aes(x=Stratification1, y=Data_Value, color=Stratification1 ) ) + 
  geom_boxplot() + 
  facet_wrap(~ YearStart) +
  geom_smooth(method = lm, se = FALSE) + 
  theme(axis.text.x = element_text(angle=45)) +
  labs(title="Category Gender vs Obesity percentage", x="Gender", y="Obesity percentage")
```

For this graph, we only can see male has wider range than femail. And male's mean value is higher than female.

> For "Percent of adults aged 18 years and older who have obesity" question, categories Gender, Education, Income, Age and Race/Ethnicity, which has significant impact on obesity. For example, people around 55-64, whose is male, who has lower education level and income less than $15,000, they will have higher chances to have a obesity health problem.


##### How about the obesity rate vs location(each state)

Let's group the dataset by year, location and categories

```{r message=FALSE, warning=FALSE}
q1_groupby <- q1 %>% group_by(YearStart,LocationAbbr) %>% summarise(total = sum(Data_Value) / n() )
```

```{r message=FALSE, warning=FALSE}
q1_groupby %>% ggplot(aes(x=YearStart, y=total, color = LocationAbbr)) + 
  geom_line() + 
  facet_wrap(~LocationAbbr) +
  labs(title="YearStart vs Data_Value", x="YearStart", y="Total Rate")
```

Corssing all state, we can find a kind of linear pattern. Rate is increasing year by year.

## Unsupervised Learning - build a clustering model

### Build Kmeams Model #1

#### Let's build a simple clustering model. We need to convert factor to numeric

```{r message=FALSE, warning=FALSE}
q1_model_df <- q1;
q1_model_df$Sample_Size <- NULL;
q1_model_df$LocationAbbr <- as.integer(q1_model_df$LocationAbbr);
q1_model_df$Class <- as.integer(q1_model_df$Class);
q1_model_df$QuestionID <- as.integer(q1_model_df$QuestionID);
q1_model_df$Stratification1 <- as.integer(q1_model_df$Stratification1);
q1_model_df$StratificationCategory1 <- as.integer(q1_model_df$StratificationCategory1)
```

#### Scaling data

```{r message=FALSE, warning=FALSE}
q1_model_df$YearStart <- scale(q1_model_df$YearStart);
q1_model_df$Data_Value <- scale(q1_model_df$Data_Value);
q1_model_df$Stratification1 <- scale(q1_model_df$Stratification1);
str(q1_model_df)
```

#### Determine number of clusters

Run tests on various numbers of clusters to look for an "elbow" which will suggest how many useful clusters we can hope to get from this data.

```{r message=FALSE, warning=FALSE}
wss <- (nrow(q1_model_df)-1)*sum(apply(q1_model_df,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(q1_model_df,
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares") 
 
```

In terms of how many clusters we should choose, these graphs could be suggesting anywhere from 3 to 4, depending on how we read them.

#### Build a kmean model with 3 clusters:

```{r message=FALSE, warning=FALSE}
# K-Means Cluster Analysis
fit <- kmeans(q1_model_df, 3) # 3 cluster solution
```

```{r message=FALSE, warning=FALSE}
# Optional Step - We need to export a predictable model - so that, we will use library flexclust
q1_model_df_shiny_model_1 = kcca(q1_model_df, k=3, kccaFamily("kmeans"));
saveRDS(q1_model_df_shiny_model_1, file = "./cluster-model-1.rda");
image(q1_model_df_shiny_model_1);

# get cluster means
aggregate(q1_model_df,by=list(fit$cluster),FUN=mean);
# append cluster assignment
q1_model_df <- data.frame(q1_model_df, fit$cluster)
## Assess Model
```

Let's take a look how many dataset in each clusters. And what is performance of this model.

```{r message=FALSE, warning=FALSE}
q1_model_df %>% group_by(fit.cluster) %>% summarise(n());
fit
```

This model has a high cluster mean 86% (between_SS / total_SS). 86% is a measure of the total variance in your data set that is explained by the clustering.

### Build Hierarchical Model #2

#### Let's build a simple clustering model. We need to convert factor to numeric

```{r message=FALSE, warning=FALSE}
set.seed(786);
q1_h_model_df <- q1;
q1_h_model_df$Sample_Size <- NULL;
q1_h_model_df$LocationAbbr <- as.integer(q1_h_model_df$LocationAbbr);
q1_h_model_df$Class <- as.integer(q1_h_model_df$Class);
q1_h_model_df$QuestionID <- as.integer(q1_h_model_df$QuestionID);
q1_h_model_df$Stratification1 <- as.integer(q1_h_model_df$Stratification1);
q1_h_model_df$StratificationCategory1 <- as.integer(q1_h_model_df$StratificationCategory1)
```

#### Scaling data

Use R's scale() function to scale all your column values instead of one by one.

```{r message=FALSE, warning=FALSE}
q1_h_model_df <- as.data.frame(scale(q1_h_model_df))
```

#### Calculate euclidean distance

```{r message=FALSE, warning=FALSE}
q1_h_model_df_dist <- dist(q1_h_model_df, method = 'euclidean')
```

#### Build a hierarchical model

```{r message=FALSE, warning=FALSE}
hclust_avg <- hclust(q1_h_model_df_dist, method = 'average');
plot(hclust_avg);
abline(h = 3.8, col = 'red')
```


As you can see in the dengrogram if we cut the tree at 4 (h=4), we get 3 clusters. 
However, we can see there is 2 big clusters just right under around h=3.8, if cut the tree at 3.8, we would get 4 clusters. And it also match the "elbow" chart.
In the 1st model, we choose k=3. In this dengrogram, let's cut the tree at 3.8, so that, we will get 4 clusters.

```{r message=FALSE, warning=FALSE}
cut_avg <- cutree(hclust_avg, h = 3.8);
plot(hclust_avg);
rect.hclust(hclust_avg , k = 4, border = 2:6);
abline(h = 3.8, col = 'red')

```

However, we can see the 2nd cluster(between green and blue) one, which is too small. It might make sence we should only use k=3 rather than 4.


# Conclusions

We wrangled this large, complicated, messy dataset into a much more compact entity representing only data we are interested in. Through our exploration of the data, we were able to see the widely-known correlations between a number of factors and obesity rate. These factors include age, gender, education, ethnic background, income, quality of diet, and quantity of exercise.

Through unsupervised learning, we discovered that some demographics appear to set themselves apart, both in terms of demographic and behavioural features, and observed obesity rates. We discovered which features were most strongly correlated with obesity overall. This information can be useful both for predicting which groups stand most to benefit from behavioural changes, as well as giving us clues about which behavioural changes are most relevant to those groups.

The knowledge that was discovered in this process, in combination with the problem we set out to solve, contributed to the design and development of the user interface for our Shiny app, which can be found here: https://ptgray.shinyapps.io/project/

# Recommendations

A predictive model, based on supervised learning using obesity rate as the value to predict, is essential to the completion of this project and is the only recommendation we have at this time.

Any such model, with the code that was used to generate it, would be a fabulous addition to this project. Without a doubt.

Ideally, we would train, evaluate, and optimize a model that can predict obesity rate for a given demographic (age, gender, level of education) using known values for Exercise and Eating Scores. We would validate this predicted obesity rate by comparing it to the known value of the obesity rate for that group. We would then predict a selection of hypothetical obesity rates for this demographic using artifical values representing modest (~10%) improvements in a selection of behaviours. This would give us insights into which behaviours, or combination of behaviours, would be of most value in an obesity reduction campaign. Finally, we would compare these theoretical obesity rates with the observed obesity rate for the comparable demographic, but with a higher level of education. From our exploration, there seems to be a good chance that the message "stay in school!" will be of more utility than any tinkering with people's lifestyles after the fact.

.

### CSML1000 York University "Machine Learning in the Business Context". Fall 2019

### Final Project, Group 8

Tamer Hanna <tamerh@my.yorku.ca>
Pete Gray <ptgray@my.yorku.ca>
Xiaohai Lu <yu271637@my.yorku.ca>
Haofeng Zhou <zhf85@my.yorku.ca>

.